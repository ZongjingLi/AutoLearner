{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/melkor/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 128, 128, 3])\n",
      "torch.Size([65536]) torch.Size([65536])\n",
      "torch.Size([2198]) torch.Size([2198])\n",
      "level:0 torch.Size([4, 128, 128, 3])\n",
      "level:1 torch.Size([4, 128, 128, 3])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from models import *\n",
    "from config import *\n",
    "\n",
    "from datasets import *\n",
    "\n",
    "import math\n",
    "\n",
    "dataset = ToyData(\"train\")\n",
    "\n",
    "idx = [4, 7, 9, 11]\n",
    "ims = torch.cat([dataset[i][\"image\"].unsqueeze(0) for i in idx])\n",
    "\n",
    "print(ims.shape)\n",
    "\n",
    "percept = PSGNet(config.imsize,3)\n",
    "\n",
    "\n",
    "outputs = percept(ims)\n",
    "\n",
    "def normalize_outputs(outputs):\n",
    "    recons = outputs[\"recons\"]\n",
    "    B = recons[0].shape[0]\n",
    "    W = int(math.sqrt(recons[0].shape[1]))\n",
    "\n",
    "    clusters  = outputs[\"clusters\"]\n",
    "    features  = outputs[\"features\"]\n",
    "    centroids = outputs[\"centroids\"]\n",
    "    moments   = outputs[\"moments\"]\n",
    "\n",
    "    for item in clusters:print(item[0].shape, item[1].shape)\n",
    "\n",
    "    level_reconstructions = [item.reshape([B,W,W,3]) for item in recons]\n",
    "\n",
    "    return {\"recons\":level_reconstructions, \n",
    "    \"clusters\": clusters, \n",
    "    \"features\": features, \n",
    "    \"centroids\": centroids, \n",
    "    \"moments\": moments,\n",
    "    \"batch\":outputs[\"batch\"]}\n",
    "\n",
    "outputs = normalize_outputs(outputs)\n",
    "recons = outputs[\"recons\"]\n",
    "\n",
    "for i,item in enumerate(recons):print(\"level:{}\".format(i),item.shape)\n",
    "\n",
    "\n",
    "\n",
    "class AbstractNet(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_heads =8\n",
    "        self.feature_decoder = nn.Transformer(nhead=16, num_encoder_layers=12,d_model = config.global_feature_dim,batch_first = True)\n",
    "        self.spatial_decoder = nn.Transformer(nhead=16, num_encoder_layers=12,d_model = config.global_feature_dim,batch_first = True)\n",
    "        self.source_heads = nn.Parameter(torch.randn([self.num_heads,config.global_feature_dim]))\n",
    "\n",
    "        self.coordinate_decoder = nn.Linear(config.global_feature_dim, 2)\n",
    "    \n",
    "    def forward(self, feature, spatial):\n",
    "        B, M, C = feature.shape\n",
    "        N = self.num_heads\n",
    "        # [Feature Propagation]\n",
    "        component_features = feature\n",
    "        component_spaitals = spatial\n",
    "\n",
    "        # [Decode Proposals]\n",
    "        global_feature = torch.randn()\n",
    "        source_heads = self.souce_heads\n",
    "        feature_proposals = self.feature_decoder(source_heads,global_feature)\n",
    "        spaital_proposals = self.spatial_decoder(source_heads,global_feature)\n",
    "\n",
    "        # [Component Matching]\n",
    "        # component_features : [B,M,C]\n",
    "        # feature_proposals  : [B,N,C]\n",
    "\n",
    "        match = torch.softmax(torch.einsum(\"bnc,bmc -> bnm\",component_features, proposal_features)/math.sqrt(C), dim = -1)\n",
    "        existence = torch.max(match, dim = 1).values  # [B, N, 1]\n",
    "\n",
    "        # [Construct Representation]\n",
    "        output_graph = 0\n",
    "\n",
    "        return output_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clusters\n",
      "torch.Size([65536]) torch.Size([65536])\n",
      "torch.Size([2198]) torch.Size([2198])\n",
      "features\n",
      "torch.Size([2198, 64])\n",
      "torch.Size([1021, 64])\n",
      "centroids\n",
      "torch.Size([2198, 2])\n",
      "torch.Size([1021, 2])\n",
      "moments\n",
      "torch.Size([2198, 2])\n",
      "torch.Size([1021, 2])\n"
     ]
    }
   ],
   "source": [
    "print(\"clusters\")\n",
    "clusters = outputs[\"clusters\"]\n",
    "for item in clusters:\n",
    "    nodes = item[0]\n",
    "    batch = item[1]\n",
    "    print(nodes.shape, batch.shape)\n",
    "\n",
    "\n",
    "print(\"features\")\n",
    "features = outputs[\"features\"]\n",
    "for item in features:\n",
    "    print(item.shape)\n",
    "\n",
    "\n",
    "print(\"centroids\")\n",
    "features = outputs[\"centroids\"]\n",
    "for item in features:\n",
    "    print(item.shape)\n",
    "\n",
    "print(\"moments\")\n",
    "features = outputs[\"moments\"]\n",
    "for item in features:\n",
    "    print(item.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dense_features(outputs, size = 128):\n",
    "    level_batch = outputs[\"batch\"]\n",
    "    features = outputs[\"features\"]\n",
    "    centroids = outputs[\"centroids\"]\n",
    "    moments = outputs[\"moments\"]\n",
    "    clusters = outputs[\"clusters\"]\n",
    "    level_features = []\n",
    "    for i in range(len(features)):\n",
    "        sparse_feature = features[i]\n",
    "        sparse_centroid = centroids[i]\n",
    "        sparse_moment = moments[i]\n",
    "    \n",
    "        cast_batch = level_batch[i]\n",
    "\n",
    "        feature,  batch = to_dense_batch(features[i],cast_batch)\n",
    "        centroid, batch = to_dense_batch(centroids[i],cast_batch)\n",
    "        moment,   _batch = to_dense_batch(moments[i],cast_batch)\n",
    "\n",
    "        \n",
    "        cluster_r = clusters[i][0]; \n",
    "        batch_r = clusters[i][1]\n",
    "        for cluster_j,batch_j in reversed(clusters[:i]):\n",
    "            cluster_r = cluster_r[cluster_j]\n",
    "            batch_r = batch_r[batch_j]\n",
    "\n",
    "        cluster_size = int(cluster_r.max()) + 1\n",
    "        batch_size = int(cast_batch.max()) + 1\n",
    "        local_masks = torch.zeros([batch_size*size * size, cluster_size])\n",
    "\n",
    "        local_masks[cluster_r] = 1.0\n",
    "        local_masks,batch = to_dense_batch(local_masks,batch_r)\n",
    "        local_masks = local_masks[batch]\n",
    "        print(local_masks.shape,batch.shape)\n",
    "\n",
    "        local_masks = local_masks.reshape([batch_size, size, size, cluster_size]).permute([0,3,1,2])\n",
    "        print(local_masks.shape,batch.shape)\n",
    "        #print(cluster_r.shape)\n",
    "\n",
    "\n",
    "        level_features.append({\"features\":feature, \"centroids\":centroid, \"moments\":moment,\"masks\":_batch.int(),\"local_masks\":local_masks})\n",
    "    return level_features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([65536, 2198]) torch.Size([4, 16384])\n",
      "torch.Size([4, 2198, 128, 128]) torch.Size([4, 16384])\n",
      "torch.Size([65536, 1021]) torch.Size([4, 65536])\n",
      "torch.Size([4, 1021, 128, 128]) torch.Size([4, 65536])\n"
     ]
    }
   ],
   "source": [
    "psg_features = to_dense_features(outputs)\n",
    "\n",
    "base_graph = psg_features[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEcAAAA/CAYAAACywdWzAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAC2UlEQVR4nO2aP2sVQRRHz89IeqMQQhSjkMYyBI2NnRBtbLUxhZDKD5DgJ9BSEDFFsNPWNBI0H0Bio0bh5Y+NkWiwEaxUuBY7wksM923e23m7wj2wvJlhd2ZymJ2Z3FmZGcHBHKm7A00m5DiEHIeQ4xByHEKOQxY5kqYltSRtSprL0UY/UNX7HEkDwDpwGdgGVoEbZvah0ob6QI6Rcx7YNLOPZvYTeApcy9BOdo5mqHMU+NSW3wYueA9I6vs23czU6Z4cckohaRaYrav9MuSQ8xk41ZY/mcr2YGYLwALUM3LKkGPOWQXGJZ2RNAhcB5YytJOdykeOmf2WdBtYBgaARTN7X3U7/aDypbyrTjR0Qo4dskPIcQg5DiHHIeQ4hByHkOMQchxCjkPIcQg5DiHHIeQ4dJQjaVHSrqS1trIhSS8kbaTfY6lcku6nU4e3kiZydj43ZUbOY2B6X9kcsGJm48BKygNcAcbTNQs8rKabNWFmHS9gDFhry7eAkZQeAVop/YjiGOaf+zrUb/2+yvzd3c45w2a2k9JfgOGUPujkYbTLNmqn5zCpmVk3kbz/4fSh25HzVdIIQPrdTeWlTh6gOH0ws0kzm+yyD9npVs4SMJPSM8CztvKbadWaAr63vX7/HyUmyyfADvCLYg65BRynWKU2gJfAULpXwANgC3gHTJac8Bs5Icfpg0PskB1qOyvfxw+KPVHVnAC+HVB+uszDTZHTyrFqSXrdS73xWjmEHIemyFloYr2NWMqbSlNGTiOpXU4vn+VmD8SV2Ubnuig+btoCzgKDwBvg3CGevwRMsDfWdA+YS+k54G5KXwWeU/yLMwW86lh/zXIuAstt+Xlg/pB1jJEpEFf3a5UjOFZZIK5uOVmxYoh0vRzXLad0cOwQ9ByI+0vdcnJ8lltdIK7OCbltFVmnWLXuHPLZrIG42CE71P1aNZqQ4xByHEKOQ8hxCDkOIcch5Dj8AauZo6b+NH4FAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEcAAAA/CAYAAACywdWzAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAC2UlEQVR4nO2aP2sVQRRHz89IeqMQQhSjkMYyBI2NnRBtbLUxhZDKD5DgJ9BSEDFFsNPWNBI0H0Bio0bh5Y+NkWiwEaxUuBY7wksM923e23m7wj2wvJlhd2ZymJ2Z3FmZGcHBHKm7A00m5DiEHIeQ4xByHEKOQxY5kqYltSRtSprL0UY/UNX7HEkDwDpwGdgGVoEbZvah0ob6QI6Rcx7YNLOPZvYTeApcy9BOdo5mqHMU+NSW3wYueA9I6vs23czU6Z4cckohaRaYrav9MuSQ8xk41ZY/mcr2YGYLwALUM3LKkGPOWQXGJZ2RNAhcB5YytJOdykeOmf2WdBtYBgaARTN7X3U7/aDypbyrTjR0Qo4dskPIcQg5DiHHIeQ4hByHkOMQchxCjkPIcQg5DiHHIeQ4dJQjaVHSrqS1trIhSS8kbaTfY6lcku6nU4e3kiZydj43ZUbOY2B6X9kcsGJm48BKygNcAcbTNQs8rKabNWFmHS9gDFhry7eAkZQeAVop/YjiGOaf+zrUb/2+yvzd3c45w2a2k9JfgOGUPujkYbTLNmqn5zCpmVk3kbz/4fSh25HzVdIIQPrdTeWlTh6gOH0ws0kzm+yyD9npVs4SMJPSM8CztvKbadWaAr63vX7/HyUmyyfADvCLYg65BRynWKU2gJfAULpXwANgC3gHTJac8Bs5Icfpg0PskB1qOyvfxw+KPVHVnAC+HVB+uszDTZHTyrFqSXrdS73xWjmEHIemyFloYr2NWMqbSlNGTiOpXU4vn+VmD8SV2Ubnuig+btoCzgKDwBvg3CGevwRMsDfWdA+YS+k54G5KXwWeU/yLMwW86lh/zXIuAstt+Xlg/pB1jJEpEFf3a5UjOFZZIK5uOVmxYoh0vRzXLad0cOwQ9ByI+0vdcnJ8lltdIK7OCbltFVmnWLXuHPLZrIG42CE71P1aNZqQ4xByHEKOQ8hxCDkOIcch5Dj8AauZo6b+NH4FAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEcAAAA/CAYAAACywdWzAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAC2UlEQVR4nO2aP2sVQRRHz89IeqMQQhSjkMYyBI2NnRBtbLUxhZDKD5DgJ9BSEDFFsNPWNBI0H0Bio0bh5Y+NkWiwEaxUuBY7wksM923e23m7wj2wvJlhd2ZymJ2Z3FmZGcHBHKm7A00m5DiEHIeQ4xByHEKOQxY5kqYltSRtSprL0UY/UNX7HEkDwDpwGdgGVoEbZvah0ob6QI6Rcx7YNLOPZvYTeApcy9BOdo5mqHMU+NSW3wYueA9I6vs23czU6Z4cckohaRaYrav9MuSQ8xk41ZY/mcr2YGYLwALUM3LKkGPOWQXGJZ2RNAhcB5YytJOdykeOmf2WdBtYBgaARTN7X3U7/aDypbyrTjR0Qo4dskPIcQg5DiHHIeQ4hByHkOMQchxCjkPIcQg5DiHHIeQ4dJQjaVHSrqS1trIhSS8kbaTfY6lcku6nU4e3kiZydj43ZUbOY2B6X9kcsGJm48BKygNcAcbTNQs8rKabNWFmHS9gDFhry7eAkZQeAVop/YjiGOaf+zrUb/2+yvzd3c45w2a2k9JfgOGUPujkYbTLNmqn5zCpmVk3kbz/4fSh25HzVdIIQPrdTeWlTh6gOH0ws0kzm+yyD9npVs4SMJPSM8CztvKbadWaAr63vX7/HyUmyyfADvCLYg65BRynWKU2gJfAULpXwANgC3gHTJac8Bs5Icfpg0PskB1qOyvfxw+KPVHVnAC+HVB+uszDTZHTyrFqSXrdS73xWjmEHIemyFloYr2NWMqbSlNGTiOpXU4vn+VmD8SV2Ubnuig+btoCzgKDwBvg3CGevwRMsDfWdA+YS+k54G5KXwWeU/yLMwW86lh/zXIuAstt+Xlg/pB1jJEpEFf3a5UjOFZZIK5uOVmxYoh0vRzXLad0cOwQ9ByI+0vdcnJ8lltdIK7OCbltFVmnWLXuHPLZrIG42CE71P1aNZqQ4xByHEKOQ8hxCDkOIcch5Dj8AauZo6b+NH4FAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEcAAAA/CAYAAACywdWzAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAC2UlEQVR4nO2aP2sVQRRHz89IeqMQQhSjkMYyBI2NnRBtbLUxhZDKD5DgJ9BSEDFFsNPWNBI0H0Bio0bh5Y+NkWiwEaxUuBY7wksM923e23m7wj2wvJlhd2ZymJ2Z3FmZGcHBHKm7A00m5DiEHIeQ4xByHEKOQxY5kqYltSRtSprL0UY/UNX7HEkDwDpwGdgGVoEbZvah0ob6QI6Rcx7YNLOPZvYTeApcy9BOdo5mqHMU+NSW3wYueA9I6vs23czU6Z4cckohaRaYrav9MuSQ8xk41ZY/mcr2YGYLwALUM3LKkGPOWQXGJZ2RNAhcB5YytJOdykeOmf2WdBtYBgaARTN7X3U7/aDypbyrTjR0Qo4dskPIcQg5DiHHIeQ4hByHkOMQchxCjkPIcQg5DiHHIeQ4dJQjaVHSrqS1trIhSS8kbaTfY6lcku6nU4e3kiZydj43ZUbOY2B6X9kcsGJm48BKygNcAcbTNQs8rKabNWFmHS9gDFhry7eAkZQeAVop/YjiGOaf+zrUb/2+yvzd3c45w2a2k9JfgOGUPujkYbTLNmqn5zCpmVk3kbz/4fSh25HzVdIIQPrdTeWlTh6gOH0ws0kzm+yyD9npVs4SMJPSM8CztvKbadWaAr63vX7/HyUmyyfADvCLYg65BRynWKU2gJfAULpXwANgC3gHTJac8Bs5Icfpg0PskB1qOyvfxw+KPVHVnAC+HVB+uszDTZHTyrFqSXrdS73xWjmEHIemyFloYr2NWMqbSlNGTiOpXU4vn+VmD8SV2Ubnuig+btoCzgKDwBvg3CGevwRMsDfWdA+YS+k54G5KXwWeU/yLMwW86lh/zXIuAstt+Xlg/pB1jJEpEFf3a5UjOFZZIK5uOVmxYoh0vRzXLad0cOwQ9ByI+0vdcnJ8lltdIK7OCbltFVmnWLXuHPLZrIG42CE71P1aNZqQ4xByHEKOQ8hxCDkOIcch5Dj8AauZo6b+NH4FAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEcAAAA/CAYAAACywdWzAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAC2UlEQVR4nO2aP2sVQRRHz89IeqMQQhSjkMYyBI2NnRBtbLUxhZDKD5DgJ9BSEDFFsNPWNBI0H0Bio0bh5Y+NkWiwEaxUuBY7wksM923e23m7wj2wvJlhd2ZymJ2Z3FmZGcHBHKm7A00m5DiEHIeQ4xByHEKOQxY5kqYltSRtSprL0UY/UNX7HEkDwDpwGdgGVoEbZvah0ob6QI6Rcx7YNLOPZvYTeApcy9BOdo5mqHMU+NSW3wYueA9I6vs23czU6Z4cckohaRaYrav9MuSQ8xk41ZY/mcr2YGYLwALUM3LKkGPOWQXGJZ2RNAhcB5YytJOdykeOmf2WdBtYBgaARTN7X3U7/aDypbyrTjR0Qo4dskPIcQg5DiHHIeQ4hByHkOMQchxCjkPIcQg5DiHHIeQ4dJQjaVHSrqS1trIhSS8kbaTfY6lcku6nU4e3kiZydj43ZUbOY2B6X9kcsGJm48BKygNcAcbTNQs8rKabNWFmHS9gDFhry7eAkZQeAVop/YjiGOaf+zrUb/2+yvzd3c45w2a2k9JfgOGUPujkYbTLNmqn5zCpmVk3kbz/4fSh25HzVdIIQPrdTeWlTh6gOH0ws0kzm+yyD9npVs4SMJPSM8CztvKbadWaAr63vX7/HyUmyyfADvCLYg65BRynWKU2gJfAULpXwANgC3gHTJac8Bs5Icfpg0PskB1qOyvfxw+KPVHVnAC+HVB+uszDTZHTyrFqSXrdS73xWjmEHIemyFloYr2NWMqbSlNGTiOpXU4vn+VmD8SV2Ubnuig+btoCzgKDwBvg3CGevwRMsDfWdA+YS+k54G5KXwWeU/yLMwW86lh/zXIuAstt+Xlg/pB1jJEpEFf3a5UjOFZZIK5uOVmxYoh0vRzXLad0cOwQ9ByI+0vdcnJ8lltdIK7OCbltFVmnWLXuHPLZrIG42CE71P1aNZqQ4xByHEKOQ8hxCDkOIcch5Dj8AauZo6b+NH4FAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEcAAAA/CAYAAACywdWzAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAC2UlEQVR4nO2aP2sVQRRHz89IeqMQQhSjkMYyBI2NnRBtbLUxhZDKD5DgJ9BSEDFFsNPWNBI0H0Bio0bh5Y+NkWiwEaxUuBY7wksM923e23m7wj2wvJlhd2ZymJ2Z3FmZGcHBHKm7A00m5DiEHIeQ4xByHEKOQxY5kqYltSRtSprL0UY/UNX7HEkDwDpwGdgGVoEbZvah0ob6QI6Rcx7YNLOPZvYTeApcy9BOdo5mqHMU+NSW3wYueA9I6vs23czU6Z4cckohaRaYrav9MuSQ8xk41ZY/mcr2YGYLwALUM3LKkGPOWQXGJZ2RNAhcB5YytJOdykeOmf2WdBtYBgaARTN7X3U7/aDypbyrTjR0Qo4dskPIcQg5DiHHIeQ4hByHkOMQchxCjkPIcQg5DiHHIeQ4dJQjaVHSrqS1trIhSS8kbaTfY6lcku6nU4e3kiZydj43ZUbOY2B6X9kcsGJm48BKygNcAcbTNQs8rKabNWFmHS9gDFhry7eAkZQeAVop/YjiGOaf+zrUb/2+yvzd3c45w2a2k9JfgOGUPujkYbTLNmqn5zCpmVk3kbz/4fSh25HzVdIIQPrdTeWlTh6gOH0ws0kzm+yyD9npVs4SMJPSM8CztvKbadWaAr63vX7/HyUmyyfADvCLYg65BRynWKU2gJfAULpXwANgC3gHTJac8Bs5Icfpg0PskB1qOyvfxw+KPVHVnAC+HVB+uszDTZHTyrFqSXrdS73xWjmEHIemyFloYr2NWMqbSlNGTiOpXU4vn+VmD8SV2Ubnuig+btoCzgKDwBvg3CGevwRMsDfWdA+YS+k54G5KXwWeU/yLMwW86lh/zXIuAstt+Xlg/pB1jJEpEFf3a5UjOFZZIK5uOVmxYoh0vRzXLad0cOwQ9ByI+0vdcnJ8lltdIK7OCbltFVmnWLXuHPLZrIG42CE71P1aNZqQ4xByHEKOQ8hxCDkOIcch5Dj8AauZo6b+NH4FAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEcAAAA/CAYAAACywdWzAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAC2UlEQVR4nO2aP2sVQRRHz89IeqMQQhSjkMYyBI2NnRBtbLUxhZDKD5DgJ9BSEDFFsNPWNBI0H0Bio0bh5Y+NkWiwEaxUuBY7wksM923e23m7wj2wvJlhd2ZymJ2Z3FmZGcHBHKm7A00m5DiEHIeQ4xByHEKOQxY5kqYltSRtSprL0UY/UNX7HEkDwDpwGdgGVoEbZvah0ob6QI6Rcx7YNLOPZvYTeApcy9BOdo5mqHMU+NSW3wYueA9I6vs23czU6Z4cckohaRaYrav9MuSQ8xk41ZY/mcr2YGYLwALUM3LKkGPOWQXGJZ2RNAhcB5YytJOdykeOmf2WdBtYBgaARTN7X3U7/aDypbyrTjR0Qo4dskPIcQg5DiHHIeQ4hByHkOMQchxCjkPIcQg5DiHHIeQ4dJQjaVHSrqS1trIhSS8kbaTfY6lcku6nU4e3kiZydj43ZUbOY2B6X9kcsGJm48BKygNcAcbTNQs8rKabNWFmHS9gDFhry7eAkZQeAVop/YjiGOaf+zrUb/2+yvzd3c45w2a2k9JfgOGUPujkYbTLNmqn5zCpmVk3kbz/4fSh25HzVdIIQPrdTeWlTh6gOH0ws0kzm+yyD9npVs4SMJPSM8CztvKbadWaAr63vX7/HyUmyyfADvCLYg65BRynWKU2gJfAULpXwANgC3gHTJac8Bs5Icfpg0PskB1qOyvfxw+KPVHVnAC+HVB+uszDTZHTyrFqSXrdS73xWjmEHIemyFloYr2NWMqbSlNGTiOpXU4vn+VmD8SV2Ubnuig+btoCzgKDwBvg3CGevwRMsDfWdA+YS+k54G5KXwWeU/yLMwW86lh/zXIuAstt+Xlg/pB1jJEpEFf3a5UjOFZZIK5uOVmxYoh0vRzXLad0cOwQ9ByI+0vdcnJ8lltdIK7OCbltFVmnWLXuHPLZrIG42CE71P1aNZqQ4xByHEKOQ8hxCDkOIcch5Dj8AauZo6b+NH4FAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEcAAAA/CAYAAACywdWzAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAC2UlEQVR4nO2aP2sVQRRHz89IeqMQQhSjkMYyBI2NnRBtbLUxhZDKD5DgJ9BSEDFFsNPWNBI0H0Bio0bh5Y+NkWiwEaxUuBY7wksM923e23m7wj2wvJlhd2ZymJ2Z3FmZGcHBHKm7A00m5DiEHIeQ4xByHEKOQxY5kqYltSRtSprL0UY/UNX7HEkDwDpwGdgGVoEbZvah0ob6QI6Rcx7YNLOPZvYTeApcy9BOdo5mqHMU+NSW3wYueA9I6vs23czU6Z4cckohaRaYrav9MuSQ8xk41ZY/mcr2YGYLwALUM3LKkGPOWQXGJZ2RNAhcB5YytJOdykeOmf2WdBtYBgaARTN7X3U7/aDypbyrTjR0Qo4dskPIcQg5DiHHIeQ4hByHkOMQchxCjkPIcQg5DiHHIeQ4dJQjaVHSrqS1trIhSS8kbaTfY6lcku6nU4e3kiZydj43ZUbOY2B6X9kcsGJm48BKygNcAcbTNQs8rKabNWFmHS9gDFhry7eAkZQeAVop/YjiGOaf+zrUb/2+yvzd3c45w2a2k9JfgOGUPujkYbTLNmqn5zCpmVk3kbz/4fSh25HzVdIIQPrdTeWlTh6gOH0ws0kzm+yyD9npVs4SMJPSM8CztvKbadWaAr63vX7/HyUmyyfADvCLYg65BRynWKU2gJfAULpXwANgC3gHTJac8Bs5Icfpg0PskB1qOyvfxw+KPVHVnAC+HVB+uszDTZHTyrFqSXrdS73xWjmEHIemyFloYr2NWMqbSlNGTiOpXU4vn+VmD8SV2Ubnuig+btoCzgKDwBvg3CGevwRMsDfWdA+YS+k54G5KXwWeU/yLMwW86lh/zXIuAstt+Xlg/pB1jJEpEFf3a5UjOFZZIK5uOVmxYoh0vRzXLad0cOwQ9ByI+0vdcnJ8lltdIK7OCbltFVmnWLXuHPLZrIG42CE71P1aNZqQ4xByHEKOQ8hxCDkOIcch5Dj8AauZo6b+NH4FAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEcAAAA/CAYAAACywdWzAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAC2UlEQVR4nO2aP2sVQRRHz89IeqMQQhSjkMYyBI2NnRBtbLUxhZDKD5DgJ9BSEDFFsNPWNBI0H0Bio0bh5Y+NkWiwEaxUuBY7wksM923e23m7wj2wvJlhd2ZymJ2Z3FmZGcHBHKm7A00m5DiEHIeQ4xByHEKOQxY5kqYltSRtSprL0UY/UNX7HEkDwDpwGdgGVoEbZvah0ob6QI6Rcx7YNLOPZvYTeApcy9BOdo5mqHMU+NSW3wYueA9I6vs23czU6Z4cckohaRaYrav9MuSQ8xk41ZY/mcr2YGYLwALUM3LKkGPOWQXGJZ2RNAhcB5YytJOdykeOmf2WdBtYBgaARTN7X3U7/aDypbyrTjR0Qo4dskPIcQg5DiHHIeQ4hByHkOMQchxCjkPIcQg5DiHHIeQ4dJQjaVHSrqS1trIhSS8kbaTfY6lcku6nU4e3kiZydj43ZUbOY2B6X9kcsGJm48BKygNcAcbTNQs8rKabNWFmHS9gDFhry7eAkZQeAVop/YjiGOaf+zrUb/2+yvzd3c45w2a2k9JfgOGUPujkYbTLNmqn5zCpmVk3kbz/4fSh25HzVdIIQPrdTeWlTh6gOH0ws0kzm+yyD9npVs4SMJPSM8CztvKbadWaAr63vX7/HyUmyyfADvCLYg65BRynWKU2gJfAULpXwANgC3gHTJac8Bs5Icfpg0PskB1qOyvfxw+KPVHVnAC+HVB+uszDTZHTyrFqSXrdS73xWjmEHIemyFloYr2NWMqbSlNGTiOpXU4vn+VmD8SV2Ubnuig+btoCzgKDwBvg3CGevwRMsDfWdA+YS+k54G5KXwWeU/yLMwW86lh/zXIuAstt+Xlg/pB1jJEpEFf3a5UjOFZZIK5uOVmxYoh0vRzXLad0cOwQ9ByI+0vdcnJ8lltdIK7OCbltFVmnWLXuHPLZrIG42CE71P1aNZqQ4xByHEKOQ8hxCDkOIcch5Dj8AauZo6b+NH4FAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEcAAAA/CAYAAACywdWzAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAC2UlEQVR4nO2aP2sVQRRHz89IeqMQQhSjkMYyBI2NnRBtbLUxhZDKD5DgJ9BSEDFFsNPWNBI0H0Bio0bh5Y+NkWiwEaxUuBY7wksM923e23m7wj2wvJlhd2ZymJ2Z3FmZGcHBHKm7A00m5DiEHIeQ4xByHEKOQxY5kqYltSRtSprL0UY/UNX7HEkDwDpwGdgGVoEbZvah0ob6QI6Rcx7YNLOPZvYTeApcy9BOdo5mqHMU+NSW3wYueA9I6vs23czU6Z4cckohaRaYrav9MuSQ8xk41ZY/mcr2YGYLwALUM3LKkGPOWQXGJZ2RNAhcB5YytJOdykeOmf2WdBtYBgaARTN7X3U7/aDypbyrTjR0Qo4dskPIcQg5DiHHIeQ4hByHkOMQchxCjkPIcQg5DiHHIeQ4dJQjaVHSrqS1trIhSS8kbaTfY6lcku6nU4e3kiZydj43ZUbOY2B6X9kcsGJm48BKygNcAcbTNQs8rKabNWFmHS9gDFhry7eAkZQeAVop/YjiGOaf+zrUb/2+yvzd3c45w2a2k9JfgOGUPujkYbTLNmqn5zCpmVk3kbz/4fSh25HzVdIIQPrdTeWlTh6gOH0ws0kzm+yyD9npVs4SMJPSM8CztvKbadWaAr63vX7/HyUmyyfADvCLYg65BRynWKU2gJfAULpXwANgC3gHTJac8Bs5Icfpg0PskB1qOyvfxw+KPVHVnAC+HVB+uszDTZHTyrFqSXrdS73xWjmEHIemyFloYr2NWMqbSlNGTiOpXU4vn+VmD8SV2Ubnuig+btoCzgKDwBvg3CGevwRMsDfWdA+YS+k54G5KXwWeU/yLMwW86lh/zXIuAstt+Xlg/pB1jJEpEFf3a5UjOFZZIK5uOVmxYoh0vRzXLad0cOwQ9ByI+0vdcnJ8lltdIK7OCbltFVmnWLXuHPLZrIG42CE71P1aNZqQ4xByHEKOQ8hxCDkOIcch5Dj8AauZo6b+NH4FAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "base = psg_features[-1][\"local_masks\"][1]\n",
    "for i in range(10):\n",
    "    plt.subplot(1,10,i+1)\n",
    "    plt.cla()\n",
    "    plt.imshow(base[100+i],cmap=\"bone\")\n",
    "    plt.pause(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "260.00001000000003\n"
     ]
    }
   ],
   "source": [
    "a = [27.2086, 19.4980, 20.4351, 29.7112, 20.9281, 21.2046, 28.7723, 40.6607,\n",
    "         32.1640, 19.41741]\n",
    "\n",
    "cnt = 0\n",
    "for i in a:cnt += i\n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import time\n",
    "from torch_sparse import SparseTensor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class GraphPropagation(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_iters=15,\n",
    "                 excite=True,\n",
    "                 inhibit=True,\n",
    "                 project=False,\n",
    "                 adj_thresh=0.5):\n",
    "        super().__init__()\n",
    "        self.num_iters = num_iters\n",
    "        self.excite = excite\n",
    "        self.inhibit = inhibit\n",
    "        self.project = project\n",
    "        self.adj_thresh = adj_thresh\n",
    "\n",
    "        self.adj_e = None\n",
    "        self.adj_i = None\n",
    "        self.norm_factor_e = None\n",
    "        self.norm_factor_i = None\n",
    "        self.activation_converge = False\n",
    "\n",
    "    def preprocess_inputs(self, h0, adj, activated):\n",
    "        B, N, K = h0.shape\n",
    "\n",
    "        # activated: binary tensor indicating nodes to start propagation from\n",
    "        if activated is None: # randomly initialized at a point\n",
    "            rand_idx = torch.randint(0, N, [B, ]).to(h0.device)\n",
    "            activated = F.one_hot(rand_idx, num_classes=N).unsqueeze(-1).float()  # [BT, N, 1]\n",
    "        else:\n",
    "            activated = activated.reshape(B, N, 1)\n",
    "\n",
    "        # create excitatory and inhibitory affinities, followed by thresholding\n",
    "        if not isinstance(adj, SparseTensor): # dense tensor\n",
    "            adj = adj.reshape(B, N, N)\n",
    "            adj_e, adj_i = adj, 1.0 - adj\n",
    "            if self.adj_thresh is not None:\n",
    "                adj_e = self._threshold(adj_e, self.adj_thresh)\n",
    "                adj_i = self._threshold(adj_i, self.adj_thresh)\n",
    "            sample_mask = None\n",
    "        else: # sparse tensor\n",
    "            adj_e = adj\n",
    "            adj_i = adj.copy().set_value_(1.0 - adj.storage.value())\n",
    "\n",
    "            sample_mask = adj_e.copy()\n",
    "            sample_mask = sample_mask.set_value_(torch.ones_like(sample_mask.storage.value()))\n",
    "\n",
    "            if self.adj_thresh is not None:\n",
    "                adj_e = self._threshold_sparse_tensor(adj_e, self.adj_thresh)\n",
    "                adj_i = self._threshold_sparse_tensor(adj_i, self.adj_thresh)\n",
    "\n",
    "        return h0, adj_e, adj_i, activated, sample_mask\n",
    "\n",
    "    def forward(self, h0, adj, activated=None):\n",
    "        \"\"\"\n",
    "        Function: Graph propagation to create the plateau map representation\n",
    "        Input:\n",
    "        - h0: initial hidden states (or equivalently, plateau map representation) with shape [B, N Q]\n",
    "        - adj: affinity matrix with shape\n",
    "        - activated: The graph propagation will start from the activated nodes\n",
    "                   It can be None or binary tensor of shape [B, N].\n",
    "                   If None, one node will be randomly selected as being activated\n",
    "        Return:\n",
    "        - plateau_map_list: a list of plateau maps of len self.num_iters. Each plateau map has shape [B, N, Q]\n",
    "        \"\"\"\n",
    "\n",
    "        h0, adj_e, adj_i, activated, sample_mask = self.preprocess_inputs(h0, adj, activated)\n",
    "\n",
    "        h = h0.clone()\n",
    "        plateau_map_list = []\n",
    "        running_activated = activated\n",
    "        self.activation_converge = False\n",
    "\n",
    "        # start graph propagation\n",
    "        for it in range(self.num_iters):\n",
    "            h, activated, running_activated = \\\n",
    "                self.propagate(h, adj_e, adj_i, activated, running_activated, sample_mask, it)\n",
    "            plateau_map_list.append(h.reshape(h0.shape))\n",
    "\n",
    "        return plateau_map_list\n",
    "\n",
    "    def propagate(self, h, adj_e, adj_i, activated, running_activated, sample_mask, iter):\n",
    "        B, N, D = h.shape\n",
    "\n",
    "        # Graph propagation starts at a subset of activated nodes\n",
    "        # If self.activation_converge is False, i.e. not all nodes are activated, \\\n",
    "        #   we need to apply masking to the affinities and compute the normalization factor accordingly.\n",
    "        # We do so until all the nodes are activated, i.e.  self.activation_converge == True\n",
    "\n",
    "        if not self.activation_converge:\n",
    "            if isinstance(adj_e, SparseTensor):\n",
    "                # apply the activation mask on the affinity tensors\n",
    "                adj_e = adj_e.mul(activated.flatten()[None])\n",
    "                adj_i = adj_i.mul(activated.flatten()[None])\n",
    "                sample_mask = sample_mask.mul(activated.flatten()[None])\n",
    "\n",
    "            # compute the normalization factors\n",
    "            if not isinstance(adj_e, SparseTensor):\n",
    "                norm_factor_e = torch.sum(adj_e.abs() * activated, dim=-2, keepdim=True).clamp(min=1.0).detach()\n",
    "                norm_factor_i = torch.sum(adj_i.abs() * activated, dim=-2, keepdim=True).clamp(min=1.0).detach()\n",
    "            else:\n",
    "                norm_factor_e = adj_e.sum(1).reshape(B, N, 1).clamp(min=1.0).detach()\n",
    "                norm_factor_i = adj_i.sum(1).reshape(B, N, 1).clamp(min=1.0).detach()\n",
    "\n",
    "            self.norm_factor_e = norm_factor_e # [B,1,N]\n",
    "            self.norm_factor_i = norm_factor_i # [B,1,N]\n",
    "            self.adj_e = adj_e\n",
    "            self.adj_i = adj_i\n",
    "\n",
    "            self.activation_converge = activated.sum() == (B * N)\n",
    "        else: # no update is required if all the nodes are activated\n",
    "            adj_e = self.adj_e\n",
    "            adj_i = self.adj_i\n",
    "            norm_factor_e = self.norm_factor_e\n",
    "            norm_factor_i = self.norm_factor_i\n",
    "            sample_mask = None\n",
    "\n",
    "        # [Excitation]\n",
    "        if self.excite:\n",
    "            if not isinstance(adj_e, SparseTensor):\n",
    "                e_effects = torch.matmul(h.permute(0, 2, 1), adj_e * activated) / norm_factor_e\n",
    "                e_effects = e_effects.permute(0, 2, 1)\n",
    "            else:\n",
    "                e_effects = adj_e.matmul(h.reshape(B * N, D))\n",
    "                e_effects = e_effects.reshape(B, N, D) / norm_factor_e\n",
    "            h = h + e_effects\n",
    "\n",
    "        # [Inhibition]\n",
    "        if self.inhibit:\n",
    "            if not isinstance(adj_e, SparseTensor):\n",
    "                i_effects = torch.matmul(h.permute(0, 2, 1), adj_i * activated) / norm_factor_i\n",
    "                i_effects = i_effects.permute(0, 2, 1)\n",
    "            else:\n",
    "                i_effects = adj_i.matmul(h.reshape(B * N, D))\n",
    "                i_effects = i_effects.reshape(B, N, D) / norm_factor_i\n",
    "\n",
    "            proj = self._projection(h, i_effects) if self.project else i_effects\n",
    "            h = h - proj\n",
    "\n",
    "        h = self._relu_norm(h)\n",
    "\n",
    "        # [Update activated nodes]\n",
    "        if activated.sum() < B * N:\n",
    "            if not isinstance(adj_e, SparseTensor):\n",
    "                receivers = torch.max(torch.where(adj_e > adj_i, adj_e, adj_i) * activated, dim=1, keepdim=False)[0] > 0.5 # [B,N]\n",
    "            else:\n",
    "                assert sample_mask is not None\n",
    "                receivers = sample_mask.max(dim=1) > 0.5\n",
    "                receivers = receivers.reshape(B, N)\n",
    "\n",
    "            running_activated = running_activated + receivers.unsqueeze(-1).float()\n",
    "            activated = running_activated.clamp(max=1.0).detach()\n",
    "\n",
    "        return h, activated, running_activated\n",
    "\n",
    "    @staticmethod\n",
    "    def _threshold(x, thresh):\n",
    "        return x * (x > thresh).float()\n",
    "\n",
    "    @staticmethod\n",
    "    def _threshold_sparse_tensor(x, thresh):\n",
    "        row, col, value = x.coo()\n",
    "        valid = value > thresh\n",
    "        sparse_size = [x.size(0), x.size(1)]\n",
    "        output = SparseTensor(row=row[valid],col=col[valid],value=value[valid],sparse_sizes=sparse_size)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def _projection(v, u, eps=1e-12):\n",
    "        u_norm = torch.sum(u * u, -1, keepdims=True)\n",
    "        dot_prod = torch.sum(v * u, -1, keepdims=True)\n",
    "        proj = (dot_prod / (u_norm + eps)) * u\n",
    "        return proj\n",
    "\n",
    "    @staticmethod\n",
    "    def _relu_norm(x, relu=True, norm=True, eps=1e-16):\n",
    "        x = F.relu(x) if relu else x\n",
    "        x = F.normalize(x + eps, p=2.0, dim=-1, eps=max([eps, 1e-12])) if norm else x\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "torch.Size([4, 250, 64])\n"
     ]
    }
   ],
   "source": [
    "propagator = GraphPropagation(7)\n",
    "\n",
    "\n",
    "feature = torch.randn([4,250,64])\n",
    "adj = torch.ones([4,250*250, 1])\n",
    "\n",
    "plateau_map = propagator(feature, adj)\n",
    "\n",
    "print(len(plateau_map))\n",
    "print(plateau_map[-1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbstractNet(nn.Module):\n",
    "    def __init__(self,dim = 72, width = 10, iters = 10):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_heads = width\n",
    "        feature_dim = dim\n",
    "        \n",
    "\n",
    "        self.propagator = GraphPropagation(num_iters = iters)\n",
    "        self.feature_heads = nn.Parameter(torch.randn([width, dim]))\n",
    "        self.spatial_heads = nn.Parameter(torch.randn([width, 2]))\n",
    "\n",
    "        self.transfer = FCBlock(100,1,dim,dim)\n",
    "\n",
    "\n",
    "    def forward(self, input_graph):\n",
    "        # [Feature Propagation]\n",
    "        features =  input_graph[\"features\"]\n",
    "        spatials =  input_graph[\"centroids\"]\n",
    "        masks    =  input_graph[\"masks\"]\n",
    "        B, N, C = features.shape\n",
    "\n",
    "        # [Build Adjacency Matric]\n",
    "        adj = torch.ones([B,N,N,1])\n",
    "        #TODO: implement a non trivial solution!\n",
    "        #print(\"B:{} N:{} C:{}\".format(B,N,C))\n",
    "        \"\"\"\n",
    "        plateau_maps = self.propagator(features, adj)\n",
    "        features = plateau_maps[-1]\n",
    "        \"\"\"\n",
    "\n",
    "        # features after the graph propagation\n",
    "\n",
    "        # [Decode the Matching Head for the input graph]\n",
    "        # TODO: actually implement a version that is context dependent\n",
    "\n",
    "        feature_proposals = self.feature_heads.unsqueeze(0).repeat(B, 1, 1)\n",
    "        spatial_proposals = torch.sigmoid(self.spatial_heads).unsqueeze(0).repeat(B, 1, 1)\n",
    "\n",
    "        # [Component Matching]\n",
    "\n",
    "        # component_features : [B,N,C]\n",
    "        component_features = torch.cat([features, spatials], -1)\n",
    "\n",
    "        # feature_proposals  : [B,M,C]\n",
    "        proposal_features = torch.cat([feature_proposals,spatial_proposals], -1)\n",
    "\n",
    " \n",
    "        match = torch.softmax(masks.unsqueeze(-1) * torch.einsum(\"bnc,bmc -> bnm\",component_features, proposal_features)/math.sqrt(C), dim = -1)\n",
    "    \n",
    "\n",
    "        output_features = torch.einsum(\"bnc,bnm->bmc\",self.transfer(features), match)\n",
    "        #print(torch.sum(match,-2))\n",
    "\n",
    "        existence = torch.max(match, dim = 1).values\n",
    "\n",
    "        #print(\"e:\",existence)\n",
    "\n",
    "\n",
    "        out_centroids = torch.einsum(\"bnk,bnm->bmk\",spatials,match)\n",
    "\n",
    "        # calculate the local mask for visualization\n",
    "\n",
    "        #input_local_masks = input_graph[\"local_masks\"]\n",
    "        input_local_masks = torch.ones([B,N,128,128])\n",
    "        #print(input_local_masks.shape)\n",
    "        output_local_masks = torch.einsum(\"bnwh,bnm->bmwh\",input_local_masks,match)\n",
    "\n",
    "        output_graph = {\"features\":output_features, \"centroids\":out_centroids, \"masks\":existence, \"edge\":match, \"local_masks\":output_local_masks}\n",
    "        return output_graph\n",
    "\n",
    "test_net = AbstractNet(64, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B:4 N:260 C:64\n",
      "tensor([[24.2960, 46.9277, 29.1820, 20.4438, 17.7945, 14.8668, 30.9136, 22.3007,\n",
      "         30.7347, 22.5403],\n",
      "        [24.4654, 45.8139, 28.6731, 20.5793, 18.0654, 15.3204, 30.7000, 22.4795,\n",
      "         31.0686, 22.8344],\n",
      "        [24.1466, 47.2655, 28.9958, 20.1783, 17.4926, 14.6230, 31.0297, 22.1647,\n",
      "         31.5984, 22.5055],\n",
      "        [24.3572, 47.9064, 28.7659, 20.0374, 17.1996, 14.3041, 31.2534, 22.1374,\n",
      "         31.6785, 22.3602]], grad_fn=<SumBackward1>)\n",
      "e: tensor([[0.1094, 0.2487, 0.1438, 0.1000, 0.1000, 0.1000, 0.1467, 0.1000, 0.1604,\n",
      "         0.1000],\n",
      "        [0.1107, 0.2399, 0.1381, 0.1000, 0.1000, 0.1000, 0.1467, 0.1000, 0.1604,\n",
      "         0.1070],\n",
      "        [0.1033, 0.2396, 0.1450, 0.1002, 0.1000, 0.1000, 0.1464, 0.1000, 0.1604,\n",
      "         0.1000],\n",
      "        [0.1127, 0.2353, 0.1405, 0.1056, 0.1000, 0.1000, 0.1464, 0.1000, 0.1604,\n",
      "         0.1148]], grad_fn=<MaxBackward0>)\n",
      "torch.Size([4, 260, 128, 128])\n",
      "torch.Size([4, 10, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "input_graph = base_graph\n",
    "output_graph = test_net(input_graph)\n",
    "\n",
    "\n",
    "output_masks = output_graph[\"local_masks\"]\n",
    "print(output_masks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "260.00011\n"
     ]
    }
   ],
   "source": [
    "a = [24.2960, 46.9277, 29.1820, 20.4438, 17.7945, 14.8668, 30.9136, 22.3007,\n",
    "         30.7347, 22.54031]\n",
    "\n",
    "cnt = 0\n",
    "for i in a:cnt += i\n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Fact(n):\n",
    "    if n == 1 or n == 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return Fact(n-1) * n\n",
    "\n",
    "def C(n,r):\n",
    "    return Fact(n)/(Fact(n-r) * Fact(r))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1365.0"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C(16,4) - C(15,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1814400.0"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(Fact(10) * Fact(10))/(Fact(10) + Fact(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1302882537600"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1302884352000 - 1814400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117.20930232558139"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(Fact(7) * Fact(5))/(Fact(7) + Fact(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40320"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Fact(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "565722720.0"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "(Fact(17+15))/(Fact(17) * Fact(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "184756.0"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(Fact(17-7+15-5))/(Fact(17-7) * Fact(15-5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "792.0"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(Fact(7+5))/(Fact(7) * Fact(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "146326752"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "792 * 184756"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "419395968.0"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(Fact(17+15))/(Fact(17) * Fact(15)) - 792 * 184756"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1365.0"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C(16,4) - C(15,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40320"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Fact(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2401"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "7**4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3386880"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Fact(10) - Fact(8) * 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n",
      "39 8\n",
      "yes\n",
      "39 34\n",
      "yes\n",
      "39 144\n",
      "yes\n",
      "39 610\n",
      "yes\n",
      "39 2584\n",
      "yes\n",
      "39 10946\n",
      "yes\n",
      "39 46368\n",
      "yes\n",
      "39 196418\n",
      "yes\n",
      "39 832040\n",
      "yes\n",
      "39 3524578\n",
      "4613732\n"
     ]
    }
   ],
   "source": [
    "ans = 0\n",
    "\n",
    "prev1 = 1\n",
    "prev2 = 2\n",
    "nxt = prev1 + prev2\n",
    "\n",
    "while nxt < 4000000:\n",
    "    nxt = prev1 + prev2\n",
    "    prev1 = prev2\n",
    "    prev2 = nxt\n",
    "    if nxt%2==0:\n",
    "        print(\"yes\")\n",
    "        ans += nxt\n",
    "        print(i,nxt)\n",
    "\n",
    "print(ans + 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/melkor/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "pybullet build time: Oct 24 2022 03:35:20\n",
      "/Users/melkor/miniforge3/envs/Melkor/lib/python3.9/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:3191.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Namespace' object has no attribute 'nf_effect'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/melkor/Documents/GitHub/SceneGraphLearner/view.ipynb Cell 29'\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/melkor/Documents/GitHub/SceneGraphLearner/view.ipynb#ch0000028?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/melkor/Documents/GitHub/SceneGraphLearner/view.ipynb#ch0000028?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mconfig\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/melkor/Documents/GitHub/SceneGraphLearner/view.ipynb#ch0000028?line=3'>4</a>\u001b[0m model \u001b[39m=\u001b[39m AutoLearner(config)\n",
      "File \u001b[0;32m~/Documents/GitHub/SceneGraphLearner/models/autolearner.py:15\u001b[0m, in \u001b[0;36mAutoLearner.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/melkor/Documents/GitHub/SceneGraphLearner/models/autolearner.py?line=11'>12</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscenelearner \u001b[39m=\u001b[39m SceneLearner(config)\n\u001b[1;32m     <a href='file:///Users/melkor/Documents/GitHub/SceneGraphLearner/models/autolearner.py?line=13'>14</a>\u001b[0m \u001b[39m# [Intuitive Physics Module]\u001b[39;00m\n\u001b[0;32m---> <a href='file:///Users/melkor/Documents/GitHub/SceneGraphLearner/models/autolearner.py?line=14'>15</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparticle_filter \u001b[39m=\u001b[39m NeuroParticleFilter(config)\n",
      "File \u001b[0;32m~/Documents/GitHub/SceneGraphLearner/models/physics/particle_filter.py:68\u001b[0m, in \u001b[0;36mNeuroParticleFilter.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/melkor/Documents/GitHub/SceneGraphLearner/models/physics/particle_filter.py?line=64'>65</a>\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[1;32m     <a href='file:///Users/melkor/Documents/GitHub/SceneGraphLearner/models/physics/particle_filter.py?line=66'>67</a>\u001b[0m \u001b[39m# [Stepper] aka prediction module\u001b[39;00m\n\u001b[0;32m---> <a href='file:///Users/melkor/Documents/GitHub/SceneGraphLearner/models/physics/particle_filter.py?line=67'>68</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstepper \u001b[39m=\u001b[39m PropNet(config)\n\u001b[1;32m     <a href='file:///Users/melkor/Documents/GitHub/SceneGraphLearner/models/physics/particle_filter.py?line=69'>70</a>\u001b[0m \u001b[39m# [Matcher] ask observation module\u001b[39;00m\n\u001b[1;32m     <a href='file:///Users/melkor/Documents/GitHub/SceneGraphLearner/models/physics/particle_filter.py?line=70'>71</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmatcher \u001b[39m=\u001b[39m Matcher(config)\n",
      "File \u001b[0;32m~/Documents/GitHub/SceneGraphLearner/models/physics/sim/propnet.py:202\u001b[0m, in \u001b[0;36mPropNet.__init__\u001b[0;34m(self, config, residual)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/melkor/Documents/GitHub/SceneGraphLearner/models/physics/sim/propnet.py?line=199'>200</a>\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[1;32m    <a href='file:///Users/melkor/Documents/GitHub/SceneGraphLearner/models/physics/sim/propnet.py?line=200'>201</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig \u001b[39m=\u001b[39m config\n\u001b[0;32m--> <a href='file:///Users/melkor/Documents/GitHub/SceneGraphLearner/models/physics/sim/propnet.py?line=201'>202</a>\u001b[0m nf_effect \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39;49mnf_effect\n\u001b[1;32m    <a href='file:///Users/melkor/Documents/GitHub/SceneGraphLearner/models/physics/sim/propnet.py?line=202'>203</a>\u001b[0m attr_dim \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mattr_dim\n\u001b[1;32m    <a href='file:///Users/melkor/Documents/GitHub/SceneGraphLearner/models/physics/sim/propnet.py?line=203'>204</a>\u001b[0m state_dim \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mstate_dim\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Namespace' object has no attribute 'nf_effect'"
     ]
    }
   ],
   "source": [
    "from models import *\n",
    "from config import *\n",
    "\n",
    "model = AutoLearner(config)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a47e46093c771f9510c4aabf3710bfb1355e5f870a13f8c22092f45d4d23626d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 ('Melkor')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
